#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å±¥æ­´é–²è¦§ Streamlit ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
"""

import streamlit as st
import pandas as pd
import psycopg2
import json
import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import io
import base64

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å±¥æ­´ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ“",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-container {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    .success-metric {
        background-color: #d4edda;
        border-left: 4px solid #28a745;
    }
    .error-metric {
        background-color: #f8d7da;
        border-left: 4px solid #dc3545;
    }
    .info-metric {
        background-color: #d1ecf1;
        border-left: 4px solid #17a2b8;
    }
    .stDataFrame {
        border: 1px solid #e0e0e0;
        border-radius: 0.25rem;
    }
</style>
""", unsafe_allow_html=True)

class ArchiveHistoryApp:
    """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å±¥æ­´ç®¡ç†ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    def __init__(self):
        self.config = self.load_config()
        self.db_connection = None
        
    def load_config(self) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        config_path = "config/archive_config.json"
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            return config
        except FileNotFoundError:
            st.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")
            st.stop()
        except json.JSONDecodeError as e:
            st.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“: {e}")
            st.stop()
    
    def get_database_connection(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’å–å¾—"""
        if self.db_connection is None:
            try:
                db_config = self.config.get('database', {})
                self.db_connection = psycopg2.connect(
                    host=db_config.get('host', 'localhost'),
                    port=db_config.get('port', 5432),
                    database=db_config.get('database', 'archive_system'),
                    user=db_config.get('user', 'postgres'),
                    password=db_config.get('password', ''),
                    connect_timeout=db_config.get('timeout', 30)
                )
                return self.db_connection
            except Exception as e:
                st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼: {str(e)}")
                st.stop()
        return self.db_connection
    
    def search_archive_history(self, 
                             start_date: datetime.date,
                             end_date: datetime.date,
                             requester: str = "",
                             file_path: str = "",
                             limit: int = 1000,
                             offset: int = 0) -> pd.DataFrame:
        """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å±¥æ­´æ¤œç´¢"""
        try:
            conn = self.get_database_connection()
            
            # åŸºæœ¬ã‚¯ã‚¨ãƒª
            query = """
                SELECT 
                    id,
                    request_id,
                    requester,
                    request_date,
                    original_file_path,
                    s3_path,
                    archive_date,
                    file_size,
                    created_at
                FROM archive_history 
                WHERE request_date::date BETWEEN %s AND %s
            """
            params = [start_date, end_date]
            
            # ä¾é ¼è€…ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            if requester.strip():
                query += " AND requester LIKE %s"
                params.append(f"%{requester.strip()}%")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            if file_path.strip():
                query += " AND original_file_path ILIKE %s"
                params.append(f"%{file_path.strip()}%")
            
            # ã‚½ãƒ¼ãƒˆãƒ»åˆ¶é™
            query += " ORDER BY request_date DESC LIMIT %s OFFSET %s"
            params.extend([limit, offset])
            
            # å®Ÿè¡Œ
            df = pd.read_sql_query(query, conn, params=params)
            
            if not df.empty:
                # æ—¥æ™‚åˆ—ã®å¤‰æ›
                df['request_date'] = pd.to_datetime(df['request_date'])
                df['archive_date'] = pd.to_datetime(df['archive_date'])
                df['created_at'] = pd.to_datetime(df['created_at'])
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®è¡¨ç¤ºå½¢å¼å¤‰æ›
                df['file_size_mb'] = (df['file_size'] / 1024 / 1024).round(2)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®çŸ­ç¸®è¡¨ç¤ºç”¨
                df['file_path_short'] = df['original_file_path'].apply(
                    lambda x: x if len(x) <= 50 else f"{x[:47]}..."
                )
            
            return df
            
        except Exception as e:
            st.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return pd.DataFrame()
    
    def get_statistics(self, 
                      start_date: datetime.date,
                      end_date: datetime.date,
                      requester: str = "",
                      file_path: str = "") -> Dict:
        """çµ±è¨ˆæƒ…å ±å–å¾—"""
        try:
            conn = self.get_database_connection()
            
            # åŸºæœ¬ã‚¯ã‚¨ãƒª
            query = """
                SELECT 
                    COUNT(*) as total_files,
                    SUM(file_size) as total_size,
                    COUNT(DISTINCT request_id) as total_requests,
                    COUNT(DISTINCT requester) as total_requesters,
                    AVG(file_size) as avg_file_size,
                    MAX(file_size) as max_file_size,
                    MIN(request_date) as first_archive,
                    MAX(request_date) as last_archive
                FROM archive_history 
                WHERE request_date::date BETWEEN %s AND %s
            """
            params = [start_date, end_date]
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶è¿½åŠ 
            if requester.strip():
                query += " AND requester LIKE %s"
                params.append(f"%{requester.strip()}%")
            
            if file_path.strip():
                query += " AND original_file_path ILIKE %s"
                params.append(f"%{file_path.strip()}%")
            
            # å®Ÿè¡Œ
            with conn.cursor() as cursor:
                cursor.execute(query, params)
                result = cursor.fetchone()
                
                if result:
                    return {
                        'total_files': result[0] or 0,
                        'total_size': result[1] or 0,
                        'total_requests': result[2] or 0,
                        'total_requesters': result[3] or 0,
                        'avg_file_size': result[4] or 0,
                        'max_file_size': result[5] or 0,
                        'first_archive': result[6],
                        'last_archive': result[7]
                    }
                else:
                    return {
                        'total_files': 0,
                        'total_size': 0,
                        'total_requests': 0,
                        'total_requesters': 0,
                        'avg_file_size': 0,
                        'max_file_size': 0,
                        'first_archive': None,
                        'last_archive': None
                    }
                    
        except Exception as e:
            st.error(f"çµ±è¨ˆæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {}
    
    def get_requester_list(self) -> List[str]:
        """ä¾é ¼è€…ãƒªã‚¹ãƒˆå–å¾—"""
        try:
            conn = self.get_database_connection()
            query = "SELECT DISTINCT requester FROM archive_history ORDER BY requester"
            
            with conn.cursor() as cursor:
                cursor.execute(query)
                results = cursor.fetchall()
                return [row[0] for row in results]
                
        except Exception as e:
            st.error(f"ä¾é ¼è€…ãƒªã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def format_file_size(self, size_bytes: int) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        if size_bytes == 0:
            return "0 B"
        
        size_names = ["B", "KB", "MB", "GB", "TB"]
        i = 0
        size = float(size_bytes)
        
        while size >= 1024.0 and i < len(size_names) - 1:
            size /= 1024.0
            i += 1
        
        return f"{size:.1f} {size_names[i]}"
    
    def create_download_link(self, df: pd.DataFrame, filename: str, file_format: str = "excel") -> str:
        """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ä½œæˆ"""
        try:
            if file_format == "excel":
                # Excelå½¢å¼
                output = io.BytesIO()
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    # è¡¨ç¤ºç”¨ã®DataFrameã‚’ä½œæˆ
                    export_df = df.copy()
                    
                    # åˆ—åã‚’æ—¥æœ¬èªã«å¤‰æ›´
                    column_mapping = {
                        'id': 'ID',
                        'request_id': 'ä¾é ¼ID',
                        'requester': 'ä¾é ¼è€…',
                        'request_date': 'ä¾é ¼æ—¥æ™‚',
                        'original_file_path': 'å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹',
                        's3_path': 'S3ãƒ‘ã‚¹',
                        'archive_date': 'ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ—¥æ™‚',
                        'file_size': 'ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º(Bytes)',
                        'file_size_mb': 'ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º(MB)',
                        'created_at': 'ä½œæˆæ—¥æ™‚'
                    }
                    
                    # ä¸è¦ãªåˆ—ã‚’å‰Šé™¤
                    export_df = export_df.drop(['file_path_short'], axis=1, errors='ignore')
                    
                    # åˆ—åå¤‰æ›´
                    export_df = export_df.rename(columns=column_mapping)
                    
                    # Excelã«æ›¸ãè¾¼ã¿
                    export_df.to_excel(writer, sheet_name='ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å±¥æ­´', index=False)
                
                output.seek(0)
                b64 = base64.b64encode(output.read()).decode()
                
            elif file_format == "csv":
                # CSVå½¢å¼
                export_df = df.copy()
                export_df = export_df.drop(['file_path_short'], axis=1, errors='ignore')
                
                csv_buffer = io.StringIO()
                export_df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
                csv_string = csv_buffer.getvalue()
                b64 = base64.b64encode(csv_string.encode('utf-8-sig')).decode()
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ä½œæˆ
            mime_type = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet" if file_format == "excel" else "text/csv"
            
            href = f'<a href="data:{mime_type};base64,{b64}" download="{filename}">ğŸ“¥ {filename} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰</a>'
            return href
            
        except Exception as e:
            st.error(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return ""
    
    def render_header(self):
        """ãƒ˜ãƒƒãƒ€ãƒ¼æç”»"""
        st.markdown('<h1 class="main-header">ğŸ“ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å±¥æ­´ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ </h1>', unsafe_allow_html=True)
        
        # ç¾åœ¨æ™‚åˆ»è¡¨ç¤º
        current_time = datetime.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")
        st.markdown(f"<div style='text-align: center; color: #666; margin-bottom: 2rem;'>æœ€çµ‚æ›´æ–°: {current_time}</div>", 
                   unsafe_allow_html=True)
    
    def render_sidebar_filters(self):
        """ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æç”»"""
        st.sidebar.header("ğŸ” æ¤œç´¢æ¡ä»¶")
        
        # æ—¥ä»˜ç¯„å›²
        st.sidebar.subheader("æœŸé–“æŒ‡å®š")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ—¥ä»˜ï¼ˆéå»30æ—¥ï¼‰
        end_date = datetime.date.today()
        start_date = end_date - datetime.timedelta(days=30)
        
        start_date = st.sidebar.date_input(
            "é–‹å§‹æ—¥",
            value=start_date,
            max_value=end_date
        )
        
        end_date = st.sidebar.date_input(
            "çµ‚äº†æ—¥", 
            value=end_date,
            min_value=start_date
        )
        
        # ä¾é ¼è€…ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        st.sidebar.subheader("ä¾é ¼è€…")
        requester_list = self.get_requester_list()
        
        if requester_list:
            selected_requester = st.sidebar.selectbox(
                "ä¾é ¼è€…é¸æŠ",
                options=[""] + requester_list,
                index=0
            )
        else:
            selected_requester = st.sidebar.text_input("ä¾é ¼è€…ï¼ˆç¤¾å“¡ç•ªå·ï¼‰")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹æ¤œç´¢
        st.sidebar.subheader("ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢")
        file_path = st.sidebar.text_input(
            "ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰",
            placeholder="ä¾‹: project1, .txt, \\\\server\\share"
        )
        
        # è¡¨ç¤ºä»¶æ•°
        st.sidebar.subheader("è¡¨ç¤ºè¨­å®š")
        limit = st.sidebar.selectbox(
            "è¡¨ç¤ºä»¶æ•°",
            options=[100, 500, 1000, 2000],
            index=2
        )
        
        return start_date, end_date, selected_requester, file_path, limit
    
    def render_statistics(self, stats: Dict):
        """çµ±è¨ˆæƒ…å ±æç”»"""
        if not stats:
            return
            
        st.subheader("ğŸ“Š çµ±è¨ˆæƒ…å ±")
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.markdown('<div class="metric-container success-metric">', unsafe_allow_html=True)
            st.metric(
                label="ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°",
                value=f"{stats['total_files']:,}",
                help="æ¤œç´¢æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°"
            )
            st.markdown('</div>', unsafe_allow_html=True)
        
        with col2:
            st.markdown('<div class="metric-container info-metric">', unsafe_allow_html=True)
            st.metric(
                label="ç·ã‚µã‚¤ã‚º",
                value=self.format_file_size(stats['total_size']),
                help="ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆè¨ˆã‚µã‚¤ã‚º"
            )
            st.markdown('</div>', unsafe_allow_html=True)
        
        with col3:
            st.markdown('<div class="metric-container success-metric">', unsafe_allow_html=True)
            st.metric(
                label="ä¾é ¼ä»¶æ•°",
                value=f"{stats['total_requests']:,}",
                help="ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä¾é ¼ã®ç·æ•°"
            )
            st.markdown('</div>', unsafe_allow_html=True)
        
        with col4:
            st.markdown('<div class="metric-container info-metric">', unsafe_allow_html=True)
            st.metric(
                label="ä¾é ¼è€…æ•°",
                value=f"{stats['total_requesters']:,}",
                help="ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’ä¾é ¼ã—ãŸäººæ•°"
            )
            st.markdown('</div>', unsafe_allow_html=True)
        
        # è©³ç´°çµ±è¨ˆæƒ…å ±
        if stats['avg_file_size'] > 0:
            st.markdown("### ğŸ“ˆ è©³ç´°çµ±è¨ˆ")
            
            detail_col1, detail_col2 = st.columns(2)
            
            with detail_col1:
                st.write("**ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºçµ±è¨ˆ**")
                st.write(f"- å¹³å‡ã‚µã‚¤ã‚º: {self.format_file_size(stats['avg_file_size'])}")
                st.write(f"- æœ€å¤§ã‚µã‚¤ã‚º: {self.format_file_size(stats['max_file_size'])}")
            
            with detail_col2:
                if stats['first_archive'] and stats['last_archive']:
                    st.write("**æœŸé–“æƒ…å ±**")
                    st.write(f"- æœ€åˆã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–: {stats['first_archive'].strftime('%Y-%m-%d %H:%M')}")
                    st.write(f"- æœ€æ–°ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–: {stats['last_archive'].strftime('%Y-%m-%d %H:%M')}")
    
    def render_data_table(self, df: pd.DataFrame):
        """ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«æç”»"""
        if df.empty:
            st.warning("ğŸ“­ æ¤œç´¢æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return
        
        st.subheader(f"ğŸ“‹ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å±¥æ­´ ({len(df):,}ä»¶)")
        
        # è¡¨ç¤ºç”¨ã®DataFrameã‚’æº–å‚™
        display_df = df.copy()
        
        # è¡¨ç¤ºåˆ—ã®é¸æŠ
        display_columns = {
            'request_id': 'ä¾é ¼ID',
            'requester': 'ä¾é ¼è€…',
            'request_date': 'ä¾é ¼æ—¥æ™‚',
            'file_path_short': 'ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹',
            'file_size_mb': 'ã‚µã‚¤ã‚º(MB)',
            'archive_date': 'ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ—¥æ™‚'
        }
        
        # åˆ—ã‚’é¸æŠãƒ»ãƒªãƒãƒ¼ãƒ 
        display_df = display_df[list(display_columns.keys())]
        display_df = display_df.rename(columns=display_columns)
        
        # æ—¥æ™‚åˆ—ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        display_df['ä¾é ¼æ—¥æ™‚'] = display_df['ä¾é ¼æ—¥æ™‚'].dt.strftime('%Y-%m-%d %H:%M')
        display_df['ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ—¥æ™‚'] = display_df['ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ—¥æ™‚'].dt.strftime('%Y-%m-%d %H:%M')
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
        st.dataframe(
            display_df,
            use_container_width=True,
            hide_index=True
        )
        
        # è©³ç´°è¡¨ç¤ºã®å±•é–‹å¯èƒ½ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        with st.expander("ğŸ” è©³ç´°æƒ…å ±è¡¨ç¤º"):
            selected_indices = st.multiselect(
                "è©³ç´°ã‚’è¡¨ç¤ºã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’é¸æŠï¼ˆä¾é ¼IDã§é¸æŠï¼‰",
                options=df['request_id'].unique(),
                max_selections=5
            )
            
            if selected_indices:
                detail_df = df[df['request_id'].isin(selected_indices)]
                
                for _, row in detail_df.iterrows():
                    st.markdown(f"**ä¾é ¼ID: {row['request_id']}**")
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        st.write(f"- **ä¾é ¼è€…**: {row['requester']}")
                        st.write(f"- **ä¾é ¼æ—¥æ™‚**: {row['request_date'].strftime('%Y-%m-%d %H:%M:%S')}")
                        st.write(f"- **ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º**: {self.format_file_size(row['file_size'])}")
                    
                    with col2:
                        st.write(f"- **ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ—¥æ™‚**: {row['archive_date'].strftime('%Y-%m-%d %H:%M:%S')}")
                        st.write(f"- **å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹**: `{row['original_file_path']}`")
                        st.write(f"- **S3ãƒ‘ã‚¹**: `{row['s3_path']}`")
                    
                    st.markdown("---")
    
    def render_export_section(self, df: pd.DataFrame):
        """ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³æç”»"""
        if df.empty:
            return
            
        st.subheader("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        
        col1, col2 = st.columns(2)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        with col1:
            st.markdown("**Excelå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**")
            excel_filename = f"archive_history_{timestamp}.xlsx"
            excel_link = self.create_download_link(df, excel_filename, "excel")
            if excel_link:
                st.markdown(excel_link, unsafe_allow_html=True)
                st.caption("Excelãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.xlsxï¼‰å½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        
        with col2:
            st.markdown("**CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**")
            csv_filename = f"archive_history_{timestamp}.csv"
            csv_link = self.create_download_link(df, csv_filename, "csv")
            if csv_link:
                st.markdown(csv_link, unsafe_allow_html=True)
                st.caption("CSVãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆUTF-8-SIGï¼‰å½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    
    def run(self):
        """ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""
        try:
            # ãƒ˜ãƒƒãƒ€ãƒ¼
            self.render_header()
            
            # ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            start_date, end_date, requester, file_path, limit = self.render_sidebar_filters()
            
            # æ¤œç´¢å®Ÿè¡Œãƒœã‚¿ãƒ³
            if st.sidebar.button("ğŸ” æ¤œç´¢å®Ÿè¡Œ", type="primary"):
                st.rerun()
            
            # ãƒ‡ãƒ¼ã‚¿æ¤œç´¢
            with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œç´¢ä¸­..."):
                df = self.search_archive_history(
                    start_date=start_date,
                    end_date=end_date,
                    requester=requester,
                    file_path=file_path,
                    limit=limit
                )
                
                stats = self.get_statistics(
                    start_date=start_date,
                    end_date=end_date,
                    requester=requester,
                    file_path=file_path
                )
            
            # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
            self.render_statistics(stats)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
            self.render_data_table(df)
            
            # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½
            self.render_export_section(df)
            
            # ãƒ•ãƒƒã‚¿ãƒ¼
            st.markdown("---")
            st.markdown(
                "<div style='text-align: center; color: #666; font-size: 0.8rem;'>"
                "ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å±¥æ­´ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  v1.0 | "
                f"æ¤œç´¢æœŸé–“: {start_date.strftime('%Y-%m-%d')} ï½ {end_date.strftime('%Y-%m-%d')}"
                "</div>",
                unsafe_allow_html=True
            )
            
        except Exception as e:
            st.error(f"ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {str(e)}")
            st.exception(e)
        
        finally:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¯ãƒ­ãƒ¼ã‚º
            if self.db_connection:
                try:
                    self.db_connection.close()
                    self.db_connection = None
                except Exception:
                    pass

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    app = ArchiveHistoryApp()
    app.run()

if __name__ == "__main__":
    main()