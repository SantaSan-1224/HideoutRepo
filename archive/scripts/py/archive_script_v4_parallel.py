#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆæ¤œè¨¼ç‰ˆ v4 - S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸¦åˆ—å‡¦ç†ç‰ˆ
v3ã«S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®ä¸¦åˆ—å‡¦ç†æ©Ÿèƒ½ã‚’è¿½åŠ 
"""

import argparse
import csv
import datetime
import json
import logging
import os
import random
import sys
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import multiprocessing

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹
DEFAULT_CONFIG_PATH = "config/archive_config.json"

class FlushingFileHandler(logging.FileHandler):
    """æ¯å›å¼·åˆ¶çš„ã«ãƒ­ã‚°ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã™ã‚‹FileHandler"""
    
    def emit(self, record):
        super().emit(record)
        if hasattr(self, 'stream') and self.stream:
            self.stream.flush()
            # ã•ã‚‰ã«ç¢ºå®Ÿã«ã™ã‚‹ãŸã‚OS ãƒ¬ãƒ™ãƒ«ã§ã‚‚ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
            try:
                if hasattr(self.stream, 'fileno'):
                    os.fsync(self.stream.fileno())
            except (OSError, AttributeError):
                pass

class ErrorSimulator:
    """ã‚¨ãƒ©ãƒ¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.simulation_enabled = False
        self.error_rates = {
            'file_lock': 0.0,      # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ç‡
            'permission': 0.0,     # æ¨©é™ã‚¨ãƒ©ãƒ¼ç‡
            'network': 0.0,        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ç‡
            'file_missing': 0.0,   # ãƒ•ã‚¡ã‚¤ãƒ«æ¶ˆå¤±ã‚¨ãƒ©ãƒ¼ç‡
            's3_invalid': 0.0,     # S3ã‚¨ãƒ©ãƒ¼ç‡
        }
        self.simulated_errors = []
        self._lock = threading.Lock()  # v4è¿½åŠ : ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•
        
    def enable_simulation(self, error_type: str, error_rate: float):
        """ã‚¨ãƒ©ãƒ¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æœ‰åŠ¹åŒ–"""
        if error_type in self.error_rates and 0.0 <= error_rate <= 1.0:
            self.error_rates[error_type] = error_rate
            self.simulation_enabled = True
            print(f"ğŸ§ª ã‚¨ãƒ©ãƒ¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æœ‰åŠ¹: {error_type} = {error_rate*100:.1f}%")
    
    def should_simulate_error(self, error_type: str) -> bool:
        """ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã‚‹ã‹ã©ã†ã‹ã®åˆ¤å®šï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰"""
        if not self.simulation_enabled:
            return False
        
        rate = self.error_rates.get(error_type, 0.0)
        if rate > 0.0 and random.random() < rate:
            with self._lock:
                self.simulated_errors.append({
                    'type': error_type,
                    'timestamp': datetime.datetime.now().isoformat(),
                    'thread_id': threading.current_thread().ident
                })
            return True
        return False
    
    def get_simulation_stats(self) -> Dict:
        """ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ±è¨ˆã‚’å–å¾—"""
        with self._lock:
            stats = {'total_simulated': len(self.simulated_errors)}
            for error_type in self.error_rates:
                stats[f'{error_type}_count'] = len([e for e in self.simulated_errors if e['type'] == error_type])
            return stats

class ProgressTrackerV4:
    """é€²æ—è¿½è·¡ã‚¯ãƒ©ã‚¹ v4ï¼ˆä¸¦åˆ—å‡¦ç†å¯¾å¿œï¼‰"""
    
    def __init__(self, total_files: int, total_size: int):
        self.total_files = total_files
        self.total_size = total_size
        self.processed_files = 0
        self.processed_size = 0
        self.success_files = 0
        self.failed_files = 0
        self.start_time = datetime.datetime.now()
        self.current_file = ""
        self.last_update_time = time.time()
        
        # v4è¿½åŠ : ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ç”¨Lock
        self._lock = threading.Lock()
        
        # é€²æ—è¡¨ç¤ºç”¨ã®çµ±è¨ˆ
        self.upload_times = []
        self.file_sizes = []
        
        # v4 æ–°æ©Ÿèƒ½: ä¸¦åˆ—å‡¦ç†çµ±è¨ˆ
        self.concurrent_uploads = 0
        self.max_concurrent_uploads = 0
        self.thread_stats = {}  # ã‚¹ãƒ¬ãƒƒãƒ‰æ¯ã®çµ±è¨ˆ
        
        # v3 æ©Ÿèƒ½: è©³ç´°ã‚¨ãƒ©ãƒ¼åˆ†é¡ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰
        self.error_counts = {
            "æ¨©é™ã‚¨ãƒ©ãƒ¼": 0,
            "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼": 0,
            "ãƒ•ã‚¡ã‚¤ãƒ«ä¸å­˜åœ¨": 0,
            "ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯": 0,     
            "S3ã‚¨ãƒ©ãƒ¼": 0,           
            "ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³": 0,   
            "ãã®ä»–ã‚¨ãƒ©ãƒ¼": 0
        }
        
        # ã‚¨ãƒ©ãƒ¼è©³ç´°è¨˜éŒ²
        self.error_details = []
        
        # æ€§èƒ½çµ±è¨ˆ
        self.max_speed = 0.0
        self.file_completion_times = []
        
        print(f"\n{'='*80}")
        print(f"ğŸ“Š ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å‡¦ç†é–‹å§‹ (v4 - ä¸¦åˆ—å‡¦ç†å¯¾å¿œ)")
        print(f"ğŸ“ ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {self.total_files:,}")
        print(f"ğŸ’¾ ç·ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {self._format_size(self.total_size)}")
        print(f"â° é–‹å§‹æ™‚åˆ»: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*80}\n")
    
    def update_file_start(self, file_path: str, file_size: int):
        """ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹æ™‚ã®æ›´æ–°ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰"""
        with self._lock:
            self.concurrent_uploads += 1
            if self.concurrent_uploads > self.max_concurrent_uploads:
                self.max_concurrent_uploads = self.concurrent_uploads
            
            thread_id = threading.current_thread().ident
            if thread_id not in self.thread_stats:
                self.thread_stats[thread_id] = {'files': 0, 'bytes': 0, 'errors': 0}
            
            self.current_file = os.path.basename(file_path)
            self.current_file_size = file_size
            self.current_file_start_time = time.time()
            
            # é€²æ—è¡¨ç¤ºã®æ›´æ–°é–“éš”åˆ¶å¾¡ï¼ˆ0.5ç§’ã«1å›ï¼‰
            current_time = time.time()
            if current_time - self.last_update_time >= 0.5:
                self._display_progress("å‡¦ç†ä¸­")
                self.last_update_time = current_time
    
    def update_file_success(self, file_path: str, file_size: int, upload_time: float):
        """ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æˆåŠŸæ™‚ã®æ›´æ–°ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰"""
        with self._lock:
            self.processed_files += 1
            self.success_files += 1
            self.processed_size += file_size
            self.upload_times.append(upload_time)
            self.file_sizes.append(file_size)
            self.file_completion_times.append(time.time())
            self.concurrent_uploads -= 1
            
            # ã‚¹ãƒ¬ãƒƒãƒ‰çµ±è¨ˆæ›´æ–°
            thread_id = threading.current_thread().ident
            if thread_id in self.thread_stats:
                self.thread_stats[thread_id]['files'] += 1
                self.thread_stats[thread_id]['bytes'] += file_size
            
            # æœ€é«˜é€Ÿåº¦ã®æ›´æ–°
            if upload_time > 0:
                current_speed = file_size / upload_time
                if current_speed > self.max_speed:
                    self.max_speed = current_speed
            
            self._display_progress("å®Œäº†")
    
    def update_file_failure(self, file_path: str, file_size: int, error_msg: str, 
                           is_simulated: bool = False):
        """ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å¤±æ•—æ™‚ã®æ›´æ–°ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰"""
        with self._lock:
            self.processed_files += 1
            self.failed_files += 1
            self.concurrent_uploads -= 1
            
            # ã‚¹ãƒ¬ãƒƒãƒ‰çµ±è¨ˆæ›´æ–°
            thread_id = threading.current_thread().ident
            if thread_id in self.thread_stats:
                self.thread_stats[thread_id]['errors'] += 1
            
            # ã‚¨ãƒ©ãƒ¼åˆ†é¡
            error_type = self._classify_error_v3(error_msg, is_simulated)
            self.error_counts[error_type] += 1
            
            # ã‚¨ãƒ©ãƒ¼è©³ç´°è¨˜éŒ²
            error_detail = {
                'file_path': os.path.basename(file_path),
                'error_type': error_type,
                'error_message': error_msg,
                'is_simulated': is_simulated,
                'thread_id': thread_id,
                'timestamp': datetime.datetime.now().isoformat()
            }
            self.error_details.append(error_detail)
            
            status_display = f"å¤±æ•—: {error_type}"
            if is_simulated:
                status_display += " (ã‚·ãƒŸãƒ¥)"
            
            self._display_progress(status_display)
    
    def _classify_error_v3(self, error_msg: str, is_simulated: bool) -> str:
        """ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è©³ç´°åˆ†é¡"""
        if is_simulated:
            return "ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"
        
        error_lower = error_msg.lower()
        
        if any(keyword in error_lower for keyword in 
               ["sharing violation", "file is being used", "lock", "ãƒ­ãƒƒã‚¯", "ä½¿ç”¨ä¸­"]):
            return "ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯"
        elif any(keyword in error_lower for keyword in 
                ["s3", "bucket", "aws", "botocore", "endpoint"]):
            return "S3ã‚¨ãƒ©ãƒ¼"
        elif any(keyword in error_lower for keyword in 
                ["permission", "access", "æ¨©é™", "ã‚¢ã‚¯ã‚»ã‚¹"]):
            return "æ¨©é™ã‚¨ãƒ©ãƒ¼"
        elif any(keyword in error_lower for keyword in 
                ["network", "connection", "timeout", "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "æ¥ç¶š"]):
            return "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼"
        elif any(keyword in error_lower for keyword in 
                ["not found", "no such", "è¦‹ã¤ã‹ã‚‰", "å­˜åœ¨ã—ãªã„"]):
            return "ãƒ•ã‚¡ã‚¤ãƒ«ä¸å­˜åœ¨"
        else:
            return "ãã®ä»–ã‚¨ãƒ©ãƒ¼"
    
    def _display_progress(self, status: str = ""):
        """é€²æ—è¡¨ç¤ºï¼ˆv4ç‰ˆ: ä¸¦åˆ—å‡¦ç†æƒ…å ±è¿½åŠ ï¼‰"""
        # åŸºæœ¬çµ±è¨ˆè¨ˆç®—
        file_progress = (self.processed_files / self.total_files) * 100
        size_progress = (self.processed_size / self.total_size) * 100 if self.total_size > 0 else 0
        
        # çµŒéæ™‚é–“ãƒ»æ¨å®šå®Œäº†æ™‚é–“è¨ˆç®—
        elapsed = datetime.datetime.now() - self.start_time
        elapsed_seconds = elapsed.total_seconds()
        
        if self.processed_files > 0:
            avg_time_per_file = elapsed_seconds / self.processed_files
            remaining_files = self.total_files - self.processed_files
            eta_seconds = avg_time_per_file * remaining_files
            eta = str(datetime.timedelta(seconds=int(eta_seconds)))
        else:
            eta = "è¨ˆç®—ä¸­"
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ç”Ÿæˆ
        progress_bar = self._create_progress_bar(file_progress)
        
        # ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãªæ€§èƒ½çµ±è¨ˆ
        avg_speed = self._get_average_speed()
        
        # v4: ä¸¦åˆ—å‡¦ç†æƒ…å ±ã‚’å«ã‚ãŸè¡¨ç¤º
        concurrent_info = f"[{self.concurrent_uploads}ä¸¦åˆ—]" if self.concurrent_uploads > 0 else ""
        sim_count = self.error_counts.get("ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³", 0)
        sim_display = f" sim:{sim_count}" if sim_count > 0 else ""
        
        # 1è¡Œã§ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆè¡¨ç¤ºï¼ˆv4ç‰ˆï¼‰
        display_line = (f"\r{progress_bar} "
                       f"{self.processed_files}/{self.total_files} "
                       f"({file_progress:.1f}%|{size_progress:.1f}%) "
                       f"ETA:{eta} {concurrent_info} "
                       f"æˆåŠŸ:{self.success_files} å¤±æ•—:{self.failed_files}{sim_display} "
                       f"avg:{self._format_speed(avg_speed)} | "
                       f"{status[:15]:<15}")
        
        print(display_line, end="", flush=True)
    
    def _get_average_speed(self) -> float:
        """å¹³å‡é€Ÿåº¦ã®è¨ˆç®—"""
        if not self.upload_times:
            return 0.0
        
        total_time = sum(self.upload_times)
        total_size = sum(self.file_sizes)
        return total_size / total_time if total_time > 0 else 0.0
    
    def _calculate_throughput(self) -> float:
        """ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ•°/åˆ†ï¼‰"""
        if len(self.file_completion_times) < 2:
            return 0.0
        
        current_time = time.time()
        one_minute_ago = current_time - 60
        
        recent_completions = [t for t in self.file_completion_times if t >= one_minute_ago]
        
        if len(recent_completions) > 0:
            time_span = current_time - max(one_minute_ago, min(recent_completions))
            if time_span > 0:
                return (len(recent_completions) / time_span) * 60
        
        elapsed_minutes = (current_time - self.start_time.timestamp()) / 60
        if elapsed_minutes > 0:
            return len(self.file_completion_times) / elapsed_minutes
        
        return 0.0
    
    def _create_progress_bar(self, percentage: float, width: int = 20) -> str:
        """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®ç”Ÿæˆ"""
        filled = int(width * percentage / 100)
        bar = "â–ˆ" * filled + "â–‘" * (width - filled)
        return f"[{bar}]"
    
    def _format_size(self, bytes_size: int) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if bytes_size < 1024.0:
                return f"{bytes_size:.1f} {unit}"
            bytes_size /= 1024.0
        return f"{bytes_size:.1f} TB"
    
    def _format_speed(self, bytes_per_second: float) -> str:
        """è»¢é€é€Ÿåº¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        return f"{self._format_size(bytes_per_second)}/s"
    
    def print_final_summary(self, error_simulator: ErrorSimulator = None, parallel_workers: int = 1):
        """æœ€çµ‚ã‚µãƒãƒªãƒ¼ã®è¡¨ç¤ºï¼ˆv4æ‹¡å¼µç‰ˆ: ä¸¦åˆ—å‡¦ç†çµ±è¨ˆï¼‰"""
        elapsed = datetime.datetime.now() - self.start_time
        total_processed_size = sum(self.file_sizes)
        avg_speed = total_processed_size / elapsed.total_seconds() if elapsed.total_seconds() > 0 else 0
        
        print(f"\n\n")
        
        print(f"{'='*80}")
        print(f"ğŸ“Š ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼ (v4 - ä¸¦åˆ—å‡¦ç†å¯¾å¿œ)")
        print(f"{'='*80}")
        print(f"â° å‡¦ç†æ™‚é–“: {elapsed}")
        print(f"ğŸ“ å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {self.processed_files:,} / {self.total_files:,}")
        print(f"âœ… æˆåŠŸ: {self.success_files:,}")
        print(f"âŒ å¤±æ•—: {self.failed_files:,}")
        print(f"ğŸ’¾ å‡¦ç†æ¸ˆã¿ã‚µã‚¤ã‚º: {self._format_size(total_processed_size)}")
        
        # v4 æ–°æ©Ÿèƒ½: ä¸¦åˆ—å‡¦ç†çµ±è¨ˆ
        print(f"\nğŸ”„ ä¸¦åˆ—å‡¦ç†çµ±è¨ˆ:")
        print(f"   è¨­å®šä¸¦åˆ—åº¦: {parallel_workers}")
        print(f"   æœ€å¤§åŒæ™‚å®Ÿè¡Œæ•°: {self.max_concurrent_uploads}")
        print(f"   ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¹ãƒ¬ãƒƒãƒ‰æ•°: {len(self.thread_stats)}")
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰åˆ¥çµ±è¨ˆ
        if len(self.thread_stats) > 1:
            print(f"   ã‚¹ãƒ¬ãƒƒãƒ‰åˆ¥çµ±è¨ˆ:")
            for thread_id, stats in self.thread_stats.items():
                print(f"     Thread-{thread_id}: {stats['files']}ãƒ•ã‚¡ã‚¤ãƒ«, {self._format_size(stats['bytes'])}, {stats['errors']}ã‚¨ãƒ©ãƒ¼")
        
        # ã‚¨ãƒ©ãƒ¼çµ±è¨ˆ
        if self.failed_files > 0:
            print(f"\nğŸ“‹ ã‚¨ãƒ©ãƒ¼åˆ†é¡è©³ç´°:")
            for error_type, count in self.error_counts.items():
                if count > 0:
                    percentage = (count / self.failed_files) * 100
                    icon = "ğŸ§ª" if error_type == "ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³" else "âŒ"
                    print(f"   {icon} {error_type}: {count}ä»¶ ({percentage:.1f}%)")
            
            success_rate = (self.success_files / self.processed_files) * 100
            print(f"ğŸ“Š æˆåŠŸç‡: {success_rate:.1f}%")
        else:
            print(f"ğŸ“Š æˆåŠŸç‡: 100.0%")
        
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ±è¨ˆ
        if error_simulator and error_simulator.simulation_enabled:
            sim_stats = error_simulator.get_simulation_stats()
            print(f"\nğŸ§ª ã‚¨ãƒ©ãƒ¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ±è¨ˆ:")
            print(f"   ç·ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å›æ•°: {sim_stats['total_simulated']}å›")
            for error_type, rate in error_simulator.error_rates.items():
                if rate > 0:
                    actual_count = sim_stats.get(f'{error_type}_count', 0)
                    print(f"   {error_type}: è¨­å®š{rate*100:.1f}% â†’ å®Ÿéš›{actual_count}å›")
        
        # æ€§èƒ½çµ±è¨ˆ
        print(f"\nğŸ“ˆ æ€§èƒ½çµ±è¨ˆ:")
        print(f"   å¹³å‡é€Ÿåº¦: {self._format_speed(avg_speed)}")
        print(f"   æœ€é«˜é€Ÿåº¦: {self._format_speed(self.max_speed)}")
        
        if self.upload_times:
            avg_time_per_file = sum(self.upload_times) / len(self.upload_times)
            max_time = max(self.upload_times)
            min_time = min(self.upload_times)
            print(f"   ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥å‡¦ç†æ™‚é–“:")
            print(f"     å¹³å‡: {avg_time_per_file:.2f}ç§’")
            print(f"     æœ€å¤§: {max_time:.2f}ç§’")
            print(f"     æœ€å°: {min_time:.2f}ç§’")
        
        final_throughput = self._calculate_throughput()
        print(f"   æœ€çµ‚ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {final_throughput:.1f}ãƒ•ã‚¡ã‚¤ãƒ«/åˆ†")
        
        # v4: ä¸¦åˆ—å‡¦ç†åŠ¹æœã®æ¨å®š
        if parallel_workers > 1:
            sequential_estimate = elapsed.total_seconds() * parallel_workers
            improvement_ratio = sequential_estimate / elapsed.total_seconds()
            print(f"   ä¸¦åˆ—å‡¦ç†åŠ¹æœ: ç´„{improvement_ratio:.1f}å€é«˜é€ŸåŒ– (æ¨å®š)")
        
        print(f"{'='*80}\n")

class ArchiveProcessorTestV4:
    """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å‡¦ç†ã‚¯ãƒ©ã‚¹ï¼ˆæ¤œè¨¼ç‰ˆv4 - ä¸¦åˆ—å‡¦ç†å¯¾å¿œï¼‰"""
    
    def __init__(self, config_path: str = DEFAULT_CONFIG_PATH):
        self.config = self.load_config(config_path)
        self.logger = self.setup_logger()
        self.csv_errors = []
        self.progress_tracker = None
        self.error_simulator = ErrorSimulator(self.config)
        self.stats = {
            'total_files': 0,
            'processed_files': 0,
            'failed_files': 0,
            'total_size': 0,
            'start_time': None,
            'end_time': None
        }
        
    def load_config(self, config_path: str) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        default_config = {
            "logging": {
                "log_directory": "logs",
                "log_level": "INFO"
            },
            "file_server": {
                "exclude_extensions": [".tmp", ".lock", ".bak", ".archived"],
                "archived_suffix": ".archived"
            },
            "processing": {
                "max_file_size": 10737418240,
                "chunk_size": 8388608,
                "retry_count": 3,
                "parallel_workers": 0  # v4è¿½åŠ : 0=è‡ªå‹•è¨­å®š
            }
        }
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                
            for key, value in default_config.items():
                if key not in config:
                    config[key] = value
                elif isinstance(value, dict):
                    for sub_key, sub_value in value.items():
                        if sub_key not in config[key]:
                            config[key][sub_key] = sub_value
                            
            return config
        except FileNotFoundError:
            print(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™: {config_path}")
            return default_config
        except json.JSONDecodeError as e:
            print(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™: {e}")
            return default_config
        except Exception as e:
            print(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™: {e}")
            return default_config
            
    def setup_logger(self) -> logging.Logger:
        """ãƒ­ã‚°è¨­å®šã®åˆæœŸåŒ–"""
        logger = logging.getLogger('archive_processor_test_v4')
        logger.setLevel(logging.DEBUG)
        logger.handlers.clear()
        
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - [%(thread)d] - %(message)s')
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.ERROR)
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        try:
            log_config = self.config.get('logging', {})
            log_dir = Path(log_config.get('log_directory', 'logs'))
            log_dir.mkdir(exist_ok=True)
            
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            log_file = log_dir / f"archive_test_v4_{timestamp}.log"
            
            file_handler = FlushingFileHandler(log_file, encoding='utf-8')
            file_handler.setLevel(logging.DEBUG)
            file_handler.setFormatter(formatter)
            logger.addHandler(file_handler)
            
            logger.info("=" * 60)
            logger.info("ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ v4 (ä¸¦åˆ—å‡¦ç†ç‰ˆ) ãƒ­ã‚°é–‹å§‹")
            logger.info(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
            logger.info(f"é–‹å§‹æ™‚åˆ»: {datetime.datetime.now()}")
            logger.info("=" * 60)
            
            print(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {log_file}")
            
        except Exception as e:
            print(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
        
        return logger
    
    def enable_error_simulation(self, sim_args: Dict):
        """ã‚¨ãƒ©ãƒ¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®è¨­å®š"""
        for error_type, rate in sim_args.items():
            if rate > 0:
                self.error_simulator.enable_simulation(error_type, rate)
    
    def determine_parallel_workers(self, override_workers: Optional[int] = None) -> int:
        """ä¸¦åˆ—åº¦ã®æ±ºå®šï¼ˆv4æ–°æ©Ÿèƒ½ï¼‰"""
        if override_workers is not None and override_workers > 0:
            workers = override_workers
            source = "ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³æŒ‡å®š"
        else:
            config_workers = self.config.get('processing', {}).get('parallel_workers', 0)
            if config_workers > 0:
                workers = config_workers
                source = "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®š"
            else:
                # è‡ªå‹•è¨­å®š: CPUæ•° Ã— 3 (I/Oãƒã‚¦ãƒ³ãƒ‰ãªãŸã‚)
                cpu_count = multiprocessing.cpu_count()
                workers = min(cpu_count * 3, 20)  # æœ€å¤§20ä¸¦åˆ—
                source = f"è‡ªå‹•è¨­å®š (CPUæ•°{cpu_count} Ã— 3)"
        
        self.logger.info(f"ä¸¦åˆ—åº¦æ±ºå®š: {workers} ({source})")
        print(f"ğŸ”„ ä¸¦åˆ—åº¦: {workers} ({source})")
        return workers
        
    def validate_csv_input(self, csv_path: str) -> Tuple[List[str], List[Dict]]:
        """CSVèª­ã¿è¾¼ã¿ãƒ»æ¤œè¨¼å‡¦ç†"""
        self.logger.info(f"CSVèª­ã¿è¾¼ã¿é–‹å§‹: {csv_path}")
        print(f"ğŸ“„ CSVèª­ã¿è¾¼ã¿é–‹å§‹: {csv_path}")
        
        valid_directories = []
        self.csv_errors = []
        
        try:
            with open(csv_path, 'r', encoding='utf-8-sig') as f:
                lines = f.readlines()
            
            self.logger.info(f"èª­ã¿è¾¼ã¿è¡Œæ•°: {len(lines)}")
            print(f"ğŸ“Š èª­ã¿è¾¼ã¿è¡Œæ•°: {len(lines)}")
            
            for i, line in enumerate(lines):
                line_num = i + 1
                clean_line = line.strip()
                
                if not clean_line:
                    continue
                
                if i == 0 and any(keyword in clean_line.lower() for keyword in ['directory', 'path']):
                    self.logger.info(f"è¡Œ {line_num}: ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—")
                    continue
                
                path = clean_line
                validation_result = self._validate_directory_path_with_details(path)
                
                if validation_result['valid']:
                    valid_directories.append(path)
                    self.logger.info(f"è¡Œ {line_num}: æœ‰åŠ¹ãªãƒ‘ã‚¹è¿½åŠ  - {path[:50]}...")
                else:
                    error_item = {
                        'line_number': line_num,
                        'path': path,
                        'error_reason': validation_result['error_reason'],
                        'original_line': line.rstrip()
                    }
                    self.csv_errors.append(error_item)
                    self.logger.error(f"è¡Œ {line_num}: {validation_result['error_reason']} - {path[:50]}...")
            
        except Exception as e:
            error_msg = f"CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}"
            self.logger.error(error_msg)
            print(f"âŒ {error_msg}")
            return [], []
        
        self.logger.info(f"CSVèª­ã¿è¾¼ã¿å®Œäº† - æœ‰åŠ¹: {len(valid_directories)}, ã‚¨ãƒ©ãƒ¼: {len(self.csv_errors)}")
        print(f"âœ… æœ‰åŠ¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ•°: {len(valid_directories)}")
        if self.csv_errors:
            print(f"âš ï¸  ã‚¨ãƒ©ãƒ¼é …ç›®æ•°: {len(self.csv_errors)}")
        
        return valid_directories, self.csv_errors

    def _validate_directory_path_with_details(self, path: str) -> Dict:
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã®è©³ç´°æ¤œè¨¼"""
        try:
            if not path or path.strip() == '':
                return {'valid': False, 'error_reason': f'ãƒ‘ã‚¹æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}'}
        
    def _is_archived_file(self, filename: str, archived_suffix: str) -> bool:
        """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«åˆ¤å®š"""
        try:
            if filename.endswith(archived_suffix):
                self.logger.debug(f"ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—: {filename}")
                return True
            return False
        except Exception as e:
            self.logger.warning(f"ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«åˆ¤å®šã‚¨ãƒ©ãƒ¼: {filename} - {str(e)}")
            return False
    
    def collect_files(self, directories: List[str]) -> List[Dict]:
        """ãƒ•ã‚¡ã‚¤ãƒ«åé›†å‡¦ç†"""
        self.logger.info("ãƒ•ã‚¡ã‚¤ãƒ«åé›†é–‹å§‹")
        print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«åé›†é–‹å§‹...")
        
        files = []
        exclude_extensions = self.config.get('file_server', {}).get('exclude_extensions', [])
        max_file_size = self.config.get('processing', {}).get('max_file_size', 10737418240)
        archived_suffix = self.config.get('file_server', {}).get('archived_suffix', '.archived')
        
        self.logger.info(f"é™¤å¤–æ‹¡å¼µå­: {exclude_extensions}")
        self.logger.info(f"ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ¸ˆã¿ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹: {archived_suffix}")
        
        for dir_index, directory in enumerate(directories, 1):
            dir_preview = directory[:60] + "..." if len(directory) > 60 else directory
            self.logger.info(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‡¦ç†é–‹å§‹: [{dir_index}/{len(directories)}] {directory}")
            print(f"ğŸ“‚ [{dir_index}/{len(directories)}] {dir_preview}")
            
            try:
                file_count = 0
                archived_count = 0
                for root, dirs, filenames in os.walk(directory):
                    for filename in filenames:
                        file_path = os.path.join(root, filename)
                        
                        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®é™¤å¤–
                        if self._is_archived_file(filename, archived_suffix):
                            archived_count += 1
                            continue
                        
                        _, ext = os.path.splitext(filename)
                        if ext.lower() in exclude_extensions:
                            continue
                        
                        try:
                            stat_info = os.stat(file_path)
                            file_size = stat_info.st_size
                            
                            if file_size > max_file_size:
                                self.logger.debug(f"ã‚µã‚¤ã‚ºè¶…éã§ã‚¹ã‚­ãƒƒãƒ—: {file_path} ({file_size} > {max_file_size})")
                                continue
                            
                            file_info = {
                                'path': file_path,
                                'size': file_size,
                                'modified_time': datetime.datetime.fromtimestamp(stat_info.st_mtime),
                                'directory': directory
                            }
                            
                            files.append(file_info)
                            file_count += 1
                            
                        except OSError as e:
                            self.logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {file_path} - {str(e)}")
                            continue
                
                self.logger.info(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª {directory}: {file_count}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†, {archived_count}å€‹ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                print(f"   âœ… {file_count}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†")
                if archived_count > 0:
                    print(f"   ğŸ“„ {archived_count}å€‹ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                        
            except Exception as e:
                error_msg = f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‡¦ç†ã‚¨ãƒ©ãƒ¼: {directory} - {str(e)}"
                self.logger.error(error_msg)
                print(f"   âŒ {error_msg}")
                continue
        
        total_size = sum(f['size'] for f in files)
        self.logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«åé›†å®Œäº† - ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files)}, ç·ã‚µã‚¤ã‚º: {total_size}")
        print(f"\nğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«åé›†å®Œäº†")
        print(f"   ğŸ“ ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files):,}")
        print(f"   ğŸ’¾ ç·ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {self._format_size(total_size)}")
        
        return files
        
    def archive_to_s3(self, files: List[Dict], parallel_workers: int = 1) -> List[Dict]:
        """S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ï¼ˆv4ä¸¦åˆ—å‡¦ç†å¯¾å¿œï¼‰"""
        if not files:
            self.logger.warning("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            print("âš ï¸  ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return []
        
        # v4 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼åˆæœŸåŒ–
        total_size = sum(f['size'] for f in files)
        self.progress_tracker = ProgressTrackerV4(len(files), total_size)
        self.logger.info(f"S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–‹å§‹ - ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files)}, ç·ã‚µã‚¤ã‚º: {total_size}, ä¸¦åˆ—åº¦: {parallel_workers}")
        
        try:
            # S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®šæƒ…å ±ã®å–å¾—
            bucket_name = self.config['aws']['s3_bucket']
            storage_class = self._validate_storage_class(
                self.config['aws'].get('storage_class', 'STANDARD')
            )
            max_retries = self.config['processing'].get('retry_count', 3)
            
            self.logger.info(f"S3è¨­å®š - ãƒã‚±ãƒƒãƒˆ: {bucket_name}, ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹: {storage_class}, ãƒªãƒˆãƒ©ã‚¤: {max_retries}")
            
            if parallel_workers == 1:
                # ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†ï¼ˆå¾“æ¥ç‰ˆï¼‰
                self.logger.info("ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")
                return self._sequential_upload(files, bucket_name, storage_class, max_retries)
            else:
                # ä¸¦åˆ—å‡¦ç†ï¼ˆv4æ–°æ©Ÿèƒ½ï¼‰
                self.logger.info(f"ä¸¦åˆ—å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ: {parallel_workers}ä¸¦åˆ—")
                return self._parallel_upload(files, bucket_name, storage_class, max_retries, parallel_workers)
            
        except Exception as e:
            error_msg = f"S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}"
            self.logger.error(error_msg)
            print(f"\nâŒ {error_msg}")
            return [
                {
                    'file_path': f['path'],
                    'file_size': f['size'],
                    'directory': f['directory'],
                    'success': False,
                    'error': f"S3åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}",
                    's3_key': None,
                    'modified_time': f['modified_time'],
                    'upload_time': 0,
                    'is_simulated': False
                }
                for f in files
            ]
    
    def _sequential_upload(self, files: List[Dict], bucket_name: str, 
                          storage_class: str, max_retries: int) -> List[Dict]:
        """ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†ã«ã‚ˆã‚‹S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        s3_client = self._initialize_s3_client()
        results = []
        
        for i, file_info in enumerate(files, 1):
            result = self._upload_single_file(
                s3_client, file_info, bucket_name, storage_class, max_retries
            )
            results.append(result)
        
        # æœ€çµ‚ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        self.progress_tracker.print_final_summary(self.error_simulator, 1)
        
        success_count = len([r for r in results if r['success']])
        failed_count = len([r for r in results if not r['success']])
        self.logger.info(f"ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº† - æˆåŠŸ: {success_count}, å¤±æ•—: {failed_count}")
        
        return results
    
    def _parallel_upload(self, files: List[Dict], bucket_name: str, 
                        storage_class: str, max_retries: int, workers: int) -> List[Dict]:
        """ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆv4æ–°æ©Ÿèƒ½ï¼‰"""
        self.logger.info(f"ä¸¦åˆ—ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {workers}ä¸¦åˆ—")
        
        results = [None] * len(files)  # çµæœã‚’é †åºé€šã‚Šã«æ ¼ç´
        
        with ThreadPoolExecutor(max_workers=workers) as executor:
            # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¹ã‚¯ã‚’é€ä¿¡
            future_to_index = {}
            
            for i, file_info in enumerate(files):
                # å„ã‚¹ãƒ¬ãƒƒãƒ‰ç”¨ã«S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
                future = executor.submit(
                    self._upload_single_file_thread_safe, 
                    file_info, bucket_name, storage_class, max_retries
                )
                future_to_index[future] = i
            
            # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‹ã‚‰çµæœã‚’å–å¾—
            for future in as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    result = future.result()
                    results[index] = result
                except Exception as e:
                    self.logger.error(f"ä¸¦åˆ—ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµæœ
                    file_info = files[index]
                    results[index] = {
                        'file_path': file_info['path'],
                        'file_size': file_info['size'],
                        'directory': file_info['directory'],
                        'success': False,
                        'error': f"ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}",
                        's3_key': None,
                        'modified_time': file_info['modified_time'],
                        'upload_time': 0,
                        'is_simulated': False
                    }
        
        # æœ€çµ‚ã‚µãƒãƒªãƒ¼è¡¨ç¤ºï¼ˆä¸¦åˆ—å‡¦ç†ç‰ˆï¼‰
        self.progress_tracker.print_final_summary(self.error_simulator, workers)
        
        success_count = len([r for r in results if r and r['success']])
        failed_count = len([r for r in results if r and not r['success']])
        self.logger.info(f"ä¸¦åˆ—ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº† - æˆåŠŸ: {success_count}, å¤±æ•—: {failed_count}")
        
        return results
    
    def _upload_single_file_thread_safe(self, file_info: Dict, bucket_name: str, 
                                       storage_class: str, max_retries: int) -> Dict:
        """ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãªå˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        # å„ã‚¹ãƒ¬ãƒƒãƒ‰ã§ç‹¬ç«‹ã—ãŸS3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨
        s3_client = self._initialize_s3_client()
        return self._upload_single_file(s3_client, file_info, bucket_name, storage_class, max_retries)
    
    def _upload_single_file(self, s3_client, file_info: Dict, bucket_name: str, 
                           storage_class: str, max_retries: int) -> Dict:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†"""
        file_path = file_info['path']
        file_size = file_info['size']
        
        # é€²æ—è¡¨ç¤ºæ›´æ–°ï¼ˆå‡¦ç†é–‹å§‹ï¼‰
        self.progress_tracker.update_file_start(file_path, file_size)
        
        s3_key = self._generate_s3_key(file_path)
        
        # ã‚¨ãƒ©ãƒ¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ¤å®š
        is_simulated_error = False
        simulated_error_type = None
        
        if self.error_simulator.should_simulate_error('file_lock'):
            is_simulated_error = True
            simulated_error_type = 'file_lock'
        elif self.error_simulator.should_simulate_error('s3_invalid'):
            is_simulated_error = True
            simulated_error_type = 's3_invalid'
        elif self.error_simulator.should_simulate_error('file_missing'):
            is_simulated_error = True
            simulated_error_type = 'file_missing'
        
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼ã®å ´åˆ
        if is_simulated_error:
            upload_time = random.uniform(0.1, 2.0)
            time.sleep(upload_time)
            
            error_messages = {
                'file_lock': f'ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰: {file_path} ã¯ä»–ã®ãƒ—ãƒ­ã‚»ã‚¹ã§ä½¿ç”¨ä¸­ã§ã™',
                's3_invalid': f'S3ã‚¨ãƒ©ãƒ¼ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰: ç„¡åŠ¹ãªãƒã‚±ãƒƒãƒˆåã¾ãŸã¯ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã‚¨ãƒ©ãƒ¼',
                'file_missing': f'ãƒ•ã‚¡ã‚¤ãƒ«æ¶ˆå¤±ã‚¨ãƒ©ãƒ¼ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰: {file_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'
            }
            
            error_msg = error_messages.get(simulated_error_type, 'ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼')
            self.logger.warning(f"ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {error_msg}")
            
            upload_result = {'success': False, 'error': error_msg}
        else:
            # é€šå¸¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†
            upload_start_time = time.time()
            upload_result = self._upload_file_with_retry(
                s3_client, file_path, bucket_name, s3_key, storage_class, max_retries
            )
            upload_time = time.time() - upload_start_time
        
        # çµæœè¨˜éŒ²
        result = {
            'file_path': file_path,
            'file_size': file_size,
            'directory': file_info['directory'],
            'success': upload_result['success'],
            'error': upload_result.get('error'),
            's3_key': s3_key if upload_result['success'] else None,
            'modified_time': file_info['modified_time'],
            'upload_time': upload_time,
            'is_simulated': is_simulated_error
        }
        
        # ãƒ­ã‚°å‡ºåŠ›
        if upload_result['success']:
            self.logger.info(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {os.path.basename(file_path)} -> s3://{bucket_name}/{s3_key}")
        else:
            self.logger.error(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {os.path.basename(file_path)} - {upload_result.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")
        
        # é€²æ—è¡¨ç¤ºæ›´æ–°ï¼ˆå‡¦ç†å®Œäº†ï¼‰
        if upload_result['success']:
            self.progress_tracker.update_file_success(file_path, file_size, upload_time)
        else:
            self.progress_tracker.update_file_failure(
                file_path, file_size, upload_result.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼'), is_simulated_error
            )
        
        return result
    
    def _initialize_s3_client(self):
        """S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–"""
        try:
            import boto3
            from botocore.config import Config
            
            aws_config = self.config.get('aws', {})
            region = aws_config.get('region', 'ap-northeast-1').strip()
            bucket_name = aws_config.get('s3_bucket', '').strip()
            vpc_endpoint_url = aws_config.get('vpc_endpoint_url', '').strip()
            
            if not bucket_name:
                raise Exception("S3ãƒã‚±ãƒƒãƒˆåãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
            config = Config(
                region_name=region,
                retries={'max_attempts': 3, 'mode': 'adaptive'}
            )
            
            if vpc_endpoint_url:
                s3_client = boto3.client('s3', endpoint_url=vpc_endpoint_url, config=config)
            else:
                s3_client = boto3.client('s3', config=config)
            
            # æ¥ç¶šãƒ†ã‚¹ãƒˆï¼ˆåˆå›ã®ã¿ï¼‰
            try:
                s3_client.head_bucket(Bucket=bucket_name)
            except Exception as e:
                # ä¸¦åˆ—å‡¦ç†ã§ã¯æ¥ç¶šãƒ†ã‚¹ãƒˆå¤±æ•—ã‚’ãƒ¯ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¬ãƒ™ãƒ«ã§è¨˜éŒ²
                self.logger.warning(f"S3ãƒã‚±ãƒƒãƒˆæ¥ç¶šãƒ†ã‚¹ãƒˆ: {str(e)}")
            
            return s3_client
            
        except ImportError:
            raise Exception("boto3ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚pip install boto3 ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        except Exception as e:
            raise Exception(f"S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å¤±æ•—: {str(e)}")
    
    def _validate_storage_class(self, storage_class: str) -> str:
        """ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹æ¤œè¨¼ãƒ»èª¿æ•´"""
        if storage_class == 'GLACIER_DEEP_ARCHIVE':
            self.logger.info("ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹è‡ªå‹•å¤‰æ›: GLACIER_DEEP_ARCHIVE -> DEEP_ARCHIVE")
            return 'DEEP_ARCHIVE'
        
        valid_classes = ['STANDARD', 'STANDARD_IA', 'GLACIER', 'DEEP_ARCHIVE']
        
        if storage_class in valid_classes:
            return storage_class
        
        self.logger.warning(f"ç„¡åŠ¹ãªã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹ '{storage_class}' ã®ãŸã‚ 'STANDARD' ã«å¤‰æ›´")
        return 'STANDARD'

    def _generate_s3_key(self, file_path: str) -> str:
        """S3ã‚­ãƒ¼ç”Ÿæˆ"""
        try:
            normalized_path = file_path.replace('\\', '/')
            
            if normalized_path.startswith('//'):
                parts = normalized_path[2:].split('/')
                if len(parts) > 0:
                    server_name = parts[0]
                    if len(parts) > 1:
                        relative_path = '/'.join(parts[1:])
                        s3_key = f"{server_name}/{relative_path}"
                    else:
                        s3_key = f"{server_name}/root"
                else:
                    s3_key = "unknown_server/unknown_path"
            elif len(normalized_path) > 2 and normalized_path[1] == ':':
                drive_letter = normalized_path[0].lower()
                relative_path = normalized_path[3:]
                s3_key = f"local_{drive_letter}/{relative_path}"
            else:
                s3_key = f"other/{normalized_path}"
            
            s3_key = s3_key.lstrip('/')
            s3_key = '/'.join(part for part in s3_key.split('/') if part)
            
            return s3_key
            
        except Exception as e:
            self.logger.error(f"S3ã‚­ãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {file_path} - {str(e)}")
            filename = os.path.basename(file_path)
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            return f"fallback/{timestamp}/{filename}"
    
    def _upload_file_with_retry(self, s3_client, file_path: str, bucket_name: str, 
                               s3_key: str, storage_class: str, max_retries: int) -> Dict:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰"""
        
        for attempt in range(max_retries):
            try:
                self.logger.debug(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è©¦è¡Œ {attempt + 1}/{max_retries}: {s3_key}")
                
                s3_client.upload_file(
                    file_path,
                    bucket_name,
                    s3_key,
                    ExtraArgs={'StorageClass': storage_class}
                )
                
                return {'success': True, 'error': None}
                
            except FileNotFoundError:
                return {'success': False, 'error': 'ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}
                
            except PermissionError:
                return {'success': False, 'error': 'ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“'}
                
            except Exception as e:
                error_msg = str(e)
                
                if attempt == max_retries - 1:
                    return {'success': False, 'error': f'æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°åˆ°é”: {error_msg}'}
                
                self.logger.warning(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•— (è©¦è¡Œ {attempt + 1}/{max_retries}): {error_msg}")
                time.sleep(2 ** attempt)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
        
        return {'success': False, 'error': 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼'}
    
    def create_archived_files(self, results: List[Dict]) -> List[Dict]:
        """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¾Œå‡¦ç†ï¼ˆç©ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆâ†’å…ƒãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼‰"""
        self.logger.info("ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¾Œå‡¦ç†é–‹å§‹")
        print(f"ğŸ“„ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¾Œå‡¦ç†é–‹å§‹")
        
        successful_results = [r for r in results if r and r.get('success', False)]
        
        if not successful_results:
            self.logger.info("S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„ãŸã‚ã€ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¾Œå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            print("âš ï¸  S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„ãŸã‚ã€ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¾Œå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return results
        
        self.logger.info(f"ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¾Œå‡¦ç†å¯¾è±¡: {len(successful_results)}ä»¶")
        print(f"ğŸ“Š ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¾Œå‡¦ç†å¯¾è±¡: {len(successful_results)}ä»¶")
        
        archived_suffix = self.config.get('file_server', {}).get('archived_suffix', '.archived')
        processed_results = []
        
        for result in results:
            if not result or not result.get('success', False):
                processed_results.append(result)
                continue
            
            file_path = result['file_path']
            
            try:
                archived_file_path = f"{file_path}{archived_suffix}"
                
                self.logger.info(f"ç©ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {archived_file_path}")
                
                with open(archived_file_path, 'w') as f:
                    pass
                
                if not os.path.exists(archived_file_path):
                    raise Exception("ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                
                self.logger.info(f"å…ƒãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {file_path}")
                os.remove(file_path)
                
                if os.path.exists(file_path):
                    raise Exception("å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ")
                
                result['archived_file_path'] = archived_file_path
                result['archive_completed'] = True
                
            except Exception as e:
                error_msg = f"ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¾Œå‡¦ç†å¤±æ•—: {str(e)}"
                self.logger.error(f"{error_msg}: {file_path}")
                
                try:
                    if 'archived_file_path' in locals() and os.path.exists(archived_file_path):
                        os.remove(archived_file_path)
                        self.logger.info(f"ä½œæˆæ¸ˆã¿ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤: {archived_file_path}")
                except Exception as cleanup_error:
                    self.logger.warning(f"ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¤±æ•—: {cleanup_error}")
                
                result['success'] = False
                result['error'] = error_msg
                result['archive_completed'] = False
            
            processed_results.append(result)
        
        completed_count = len([r for r in processed_results if r and r.get('archive_completed', False)])
        failed_count = len([r for r in processed_results if r and r.get('success', False) and not r.get('archive_completed', False)])
        
        self.logger.info(f"ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¾Œå‡¦ç†å®Œäº†: {completed_count}ä»¶, å¤±æ•—: {failed_count}ä»¶")
        print(f"âœ… ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¾Œå‡¦ç†å®Œäº†: {completed_count}ä»¶")
        print(f"âŒ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¾Œå‡¦ç†å¤±æ•—: {failed_count}ä»¶")
        
        return processed_results
        
    def save_to_database(self, results: List[Dict]) -> None:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç™»éŒ²å‡¦ç†"""
        self.logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç™»éŒ²é–‹å§‹")
        print(f"ğŸ—„ï¸  ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç™»éŒ²é–‹å§‹")
        
        completed_results = [r for r in results if r and r.get('archive_completed', False)]
        
        if not completed_results:
            self.logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç™»éŒ²å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            print("âš ï¸  ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç™»éŒ²å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç™»éŒ²å¯¾è±¡: {len(completed_results)}ä»¶")
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç™»éŒ²å¯¾è±¡: {len(completed_results)}ä»¶")
        
        try:
            conn = self._connect_database()
            
            with conn:
                with conn.cursor() as cursor:
                    request_config = self.config.get('request', {})
                    request_id = self.request_id
                    requester = request_config.get('requester', '00000000')
                    current_time = datetime.datetime.now()
                    bucket_name = self.config.get('aws', {}).get('s3_bucket', '')
                    
                    insert_data = []
                    for result in completed_results:
                        s3_key = result.get('s3_key', '')
                        s3_url = f"s3://{bucket_name}/{s3_key}" if s3_key else ''
                        
                        record = (
                            request_id,
                            requester,
                            current_time,
                            result['file_path'],
                            s3_url,
                            current_time,
                            result['file_size']
                        )
                        insert_data.append(record)
                    
                    insert_query = """
                        INSERT INTO archive_history (
                            request_id, requester, request_date,
                            original_file_path, s3_path, archive_date, file_size
                        ) VALUES (%s, %s, %s, %s, %s, %s, %s)
                    """
                    
                    cursor.executemany(insert_query, insert_data)
                    inserted_count = cursor.rowcount
                    self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŒ¿å…¥å®Œäº†: {inserted_count}ä»¶")
                    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŒ¿å…¥å®Œäº†: {inserted_count}ä»¶")
            
            self.logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç™»éŒ²å®Œäº†")
            print(f"ğŸ—„ï¸  ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç™»éŒ²å®Œäº†")
            
        except Exception as e:
            error_msg = f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç™»éŒ²ã‚¨ãƒ©ãƒ¼: {str(e)}"
            self.logger.error(error_msg)
            print(f"âŒ {error_msg}")
            
        finally:
            try:
                if 'conn' in locals():
                    conn.close()
            except Exception:
                pass
    
    def _connect_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š"""
        try:
            import psycopg2

            db_config = self.config.get('database', {})
            
            conn_params = {
                'host': db_config.get('host', 'localhost'),
                'port': db_config.get('port', 5432),
                'database': db_config.get('database', 'archive_system'),
                'user': db_config.get('user', 'postgres'),
                'password': db_config.get('password', ''),
                'connect_timeout': db_config.get('timeout', 30)
            }
            
            self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š: {conn_params['host']}:{conn_params['port']}/{conn_params['database']}")
            
            conn = psycopg2.connect(**conn_params)
            conn.autocommit = False
            
            with conn.cursor() as cursor:
                cursor.execute("SELECT 1")
                cursor.fetchone()
            
            self.logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæˆåŠŸ")
            return conn
            
        except ImportError:
            raise Exception("psycopg2ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚pip install psycopg2-binary ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        except Exception as e:
            raise Exception(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå¤±æ•—: {str(e)}")
    
    def _format_size(self, bytes_size: int) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if bytes_size < 1024.0:
                return f"{bytes_size:.1f} {unit}"
            bytes_size /= 1024.0
        return f"{bytes_size:.1f} TB"
    
    def generate_csv_error_file(self, original_csv_path: str) -> Optional[str]:
        """CSVæ¤œè¨¼ã‚¨ãƒ©ãƒ¼ç”¨ã®ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ"""
        if not self.csv_errors:
            return None
            
        self.logger.info("CSVæ¤œè¨¼ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆé–‹å§‹")
        print(f"ğŸ“„ CSVæ¤œè¨¼ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆé–‹å§‹")
        
        try:
            log_config = self.config.get('logging', {})
            log_dir = Path(log_config.get('log_directory', 'logs'))
            log_dir.mkdir(exist_ok=True)
            
            original_path = Path(original_csv_path)
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            error_csv_path = log_dir / f"{original_path.stem}_csv_retry_{timestamp}.csv"
            
            original_header = self._get_original_csv_header(original_csv_path)
            
            with open(error_csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                writer = csv.writer(csvfile)
                
                if original_header:
                    writer.writerow([original_header])
                
                for item in self.csv_errors:
                    writer.writerow([item['path']])
            
            self.logger.info(f"CSVæ¤œè¨¼ã‚¨ãƒ©ãƒ¼ï¼ˆå†è©¦è¡Œç”¨ï¼‰ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå®Œäº†: {error_csv_path}")
            self.logger.info(f"å†è©¦è¡Œå¯¾è±¡ãƒ‘ã‚¹æ•°: {len(self.csv_errors)}")
            print(f"âœ… CSVæ¤œè¨¼ã‚¨ãƒ©ãƒ¼ï¼ˆå†è©¦è¡Œç”¨ï¼‰ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå®Œäº†: {error_csv_path}")
            print(f"ğŸ“Š å†è©¦è¡Œå¯¾è±¡ãƒ‘ã‚¹æ•°: {len(self.csv_errors)}")
            return str(error_csv_path)
            
        except Exception as e:
            error_msg = f"CSVæ¤œè¨¼ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå¤±æ•—: {str(e)}"
            self.logger.error(error_msg)
            print(f"âŒ {error_msg}")
            return None
    
    def generate_error_csv(self, failed_items: List[Dict], original_csv_path: str) -> Optional[str]:
        """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å‡¦ç†å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®å†è©¦è¡ŒCSVãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ"""
        if not failed_items:
            return None
            
        self.logger.info("ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¨ãƒ©ãƒ¼ï¼ˆå†è©¦è¡Œç”¨ï¼‰ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆé–‹å§‹")
        print(f"ğŸ“„ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¨ãƒ©ãƒ¼ï¼ˆå†è©¦è¡Œç”¨ï¼‰ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆé–‹å§‹")
        
        try:
            log_config = self.config.get('logging', {})
            log_dir = Path(log_config.get('log_directory', 'logs'))
            log_dir.mkdir(exist_ok=True)
            
            original_path = Path(original_csv_path)
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            error_csv_path = log_dir / f"{original_path.stem}_archive_retry_{timestamp}.csv"
            
            original_header = self._get_original_csv_header(original_csv_path)
            
            failed_directories = set()
            for item in failed_items:
                directory = item.get('directory')
                if directory:
                    failed_directories.add(directory)
            
            with open(error_csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                writer = csv.writer(csvfile)
                
                if original_header:
                    writer.writerow([original_header])
                
                for directory in sorted(failed_directories):
                    writer.writerow([directory])
            
            self.logger.info(f"ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¨ãƒ©ãƒ¼ï¼ˆå†è©¦è¡Œç”¨ï¼‰ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå®Œäº†: {error_csv_path}")
            self.logger.info(f"å†è©¦è¡Œå¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ•°: {len(failed_directories)}")
            self.logger.info(f"å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(failed_items)}")
            print(f"âœ… ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¨ãƒ©ãƒ¼ï¼ˆå†è©¦è¡Œç”¨ï¼‰ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå®Œäº†: {error_csv_path}")
            print(f"ğŸ“Š å†è©¦è¡Œå¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ•°: {len(failed_directories)}")
            print(f"ğŸ“Š å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(failed_items)}")
            
            error_summary = {}
            for item in failed_items:
                error_type = item.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')
                error_summary[error_type] = error_summary.get(error_type, 0) + 1
            
            self.logger.info("ã‚¨ãƒ©ãƒ¼ç†ç”±ã®å†…è¨³:")
            print(f"ğŸ“‹ ã‚¨ãƒ©ãƒ¼ç†ç”±ã®å†…è¨³:")
            for error_type, count in error_summary.items():
                self.logger.info(f"  - {error_type}: {count}ä»¶")
                print(f"   - {error_type}: {count}ä»¶")
            
            return str(error_csv_path)
            
        except Exception as e:
            error_msg = f"ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå¤±æ•—: {str(e)}"
            self.logger.error(error_msg)
            print(f"âŒ {error_msg}")
            return None
    
    def _get_original_csv_header(self, csv_path: str) -> Optional[str]:
        """å…ƒCSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’å–å¾—"""
        try:
            with open(csv_path, 'r', encoding='utf-8-sig') as f:
                first_line = f.readline().strip()
                if any(keyword in first_line.lower() for keyword in ['directory', 'path']):
                    return first_line
                return "Directory Path"
        except Exception:
            return "Directory Path"
        
    def print_statistics(self) -> None:
        """å‡¦ç†çµ±è¨ˆã®è¡¨ç¤º"""
        elapsed_time = self.stats['end_time'] - self.stats['start_time']
        
        self.logger.info("=== å‡¦ç†çµ±è¨ˆ ===")
        self.logger.info(f"å‡¦ç†æ™‚é–“: {elapsed_time}")
        self.logger.info(f"CSVæ¤œè¨¼ã‚¨ãƒ©ãƒ¼æ•°: {len(self.csv_errors)}")
        self.logger.info(f"ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {self.stats['total_files']}")
        self.logger.info(f"æˆåŠŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {self.stats['processed_files']}")
        self.logger.info(f"å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {self.stats['failed_files']}")
        self.logger.info(f"ç·ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {self.stats['total_size']:,} bytes")
        
    def run(self, csv_path: str, request_id: str, parallel_workers: Optional[int] = None) -> int:
        """ãƒ¡ã‚¤ãƒ³å‡¦ç†å®Ÿè¡Œï¼ˆv4: ä¸¦åˆ—åº¦æŒ‡å®šå¯¾å¿œï¼‰"""
        self.stats['start_time'] = datetime.datetime.now()
        self.request_id = request_id
        
        try:
            # ä¸¦åˆ—åº¦ã®æ±ºå®š
            workers = self.determine_parallel_workers(parallel_workers)
            
            self.logger.info(f"ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å‡¦ç†é–‹å§‹ - Request ID: {request_id}, ä¸¦åˆ—åº¦: {workers}")
            print(f"ğŸš€ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å‡¦ç†é–‹å§‹ (v4) - Request ID: {request_id}")
            print(f"ğŸ“„ CSV: {csv_path}")
            
            # 1. CSVèª­ã¿è¾¼ã¿ãƒ»æ¤œè¨¼
            directories, csv_errors = self.validate_csv_input(csv_path)
            
            if csv_errors:
                error_csv_path = self.generate_csv_error_file(csv_path)
                if error_csv_path:
                    self.logger.warning(f"CSVæ¤œè¨¼ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_csv_path}")
                    print(f"âš ï¸  CSVæ¤œè¨¼ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_csv_path}")
            
            if not directories:
                self.logger.error("å‡¦ç†å¯¾è±¡ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                print("âŒ å‡¦ç†å¯¾è±¡ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return 1
                
            # 2. ãƒ•ã‚¡ã‚¤ãƒ«åé›†
            files = self.collect_files(directories)
            if not files:
                self.logger.warning("å‡¦ç†å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                print("âš ï¸  å‡¦ç†å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return 0
                
            self.stats['total_files'] = len(files)
            self.stats['total_size'] = sum(f['size'] for f in files)
            
            # 3. S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆv4ä¸¦åˆ—å‡¦ç†å¯¾å¿œï¼‰
            upload_results = self.archive_to_s3(files, workers)
            
            # 4. ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¾Œå‡¦ç†
            print(f"\nğŸ“„ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¾Œå‡¦ç†é–‹å§‹...")
            processed_results = self.create_archived_files(upload_results)
            
            # 5. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç™»éŒ²
            print(f"ğŸ—„ï¸  ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç™»éŒ²é–‹å§‹...")
            self.save_to_database(processed_results)
            
            # 6. ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å‡¦ç†ã‚¨ãƒ©ãƒ¼å‡¦ç†
            failed_items = [r for r in processed_results if r and not r.get('success', False)]
            if failed_items:
                archive_error_csv = self.generate_error_csv(failed_items, csv_path)
                if archive_error_csv:
                    self.logger.warning(f"ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã™: {archive_error_csv}")
                    print(f"âš ï¸  ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã™: {archive_error_csv}")
                else:
                    self.logger.error("ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¨ãƒ©ãƒ¼CSVã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                    print(f"âŒ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¨ãƒ©ãƒ¼CSVã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            else:
                self.logger.info("å…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã•ã‚Œã¾ã—ãŸ")
                print(f"ğŸ‰ å…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã•ã‚Œã¾ã—ãŸ")
                
            # çµæœã‚µãƒãƒªãƒ¼ï¼ˆv4æ‹¡å¼µï¼‰
            successful_results = [r for r in processed_results if r and r.get('success', False)]
            failed_results = [r for r in processed_results if r and not r.get('success', False)]
            simulated_results = [r for r in processed_results if r and r.get('is_simulated', False)]
            
            self.stats['processed_files'] = len(successful_results)
            self.stats['failed_files'] = len(failed_results)
            
            self.logger.info("ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å‡¦ç†å®Œäº†")
            self.logger.info(f"æˆåŠŸ: {len(successful_results)}ä»¶")
            self.logger.info(f"å¤±æ•—: {len(failed_results)}ä»¶")
            if simulated_results:
                self.logger.info(f"ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: {len(simulated_results)}ä»¶")
            
            print(f"ğŸ‰ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å‡¦ç†å®Œäº†! (v4)")
            print(f"âœ… æˆåŠŸ: {len(successful_results)}ä»¶")
            print(f"âŒ å¤±æ•—: {len(failed_results)}ä»¶")
            if simulated_results:
                print(f"ğŸ§ª ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: {len(simulated_results)}ä»¶")
            print(f"ğŸ”„ ä¸¦åˆ—åº¦: {workers}")
            
            return 0
            
        except Exception as e:
            error_msg = f"ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            self.logger.error(error_msg)
            print(f"\nâŒ {error_msg}")
            return 1
            
        finally:
            self.stats['end_time'] = datetime.datetime.now()
            self.print_statistics()

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description='ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆæ¤œè¨¼ç‰ˆv4ï¼ˆä¸¦åˆ—å‡¦ç†å¯¾å¿œï¼‰')
    parser.add_argument('csv_path', help='å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨˜è¼‰ã—ãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    parser.add_argument('request_id', help='ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä¾é ¼ID')
    parser.add_argument('--config', default=DEFAULT_CONFIG_PATH, 
                       help=f'è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {DEFAULT_CONFIG_PATH})')
    
    # v4 æ–°æ©Ÿèƒ½: ä¸¦åˆ—åº¦æŒ‡å®š
    parser.add_argument('--parallel', type=int, metavar='N',
                       help='ä¸¦åˆ—åº¦æŒ‡å®š (1=ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«, 2ä»¥ä¸Š=ä¸¦åˆ—å‡¦ç†, 0=è‡ªå‹•è¨­å®š)')
    
    # v3 æ©Ÿèƒ½: ã‚¨ãƒ©ãƒ¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    sim_group = parser.add_argument_group('ã‚¨ãƒ©ãƒ¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³')
    sim_group.add_argument('--simulate-file-lock', type=float, metavar='RATE',
                          help='ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç‡ (0.0-1.0)')
    sim_group.add_argument('--simulate-permission', type=float, metavar='RATE',
                          help='æ¨©é™ã‚¨ãƒ©ãƒ¼ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç‡ (0.0-1.0)')
    sim_group.add_argument('--simulate-network', type=float, metavar='RATE',
                          help='ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç‡ (0.0-1.0)')
    sim_group.add_argument('--simulate-file-missing', type=float, metavar='RATE',
                          help='ãƒ•ã‚¡ã‚¤ãƒ«æ¶ˆå¤±ã‚¨ãƒ©ãƒ¼ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç‡ (0.0-1.0)')
    sim_group.add_argument('--simulate-s3-invalid', type=float, metavar='RATE',
                          help='S3ã‚¨ãƒ©ãƒ¼ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç‡ (0.0-1.0)')
    
    args = parser.parse_args()
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    if not os.path.exists(args.csv_path):
        print(f"âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.csv_path}")
        sys.exit(1)
    
    print(f"ğŸ” ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆæ¤œè¨¼ç‰ˆ v4")
    print(f"ğŸ“‹ æ©Ÿèƒ½: ä¸¦åˆ—å‡¦ç†ãƒ»ã‚¨ãƒ©ãƒ¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»è©³ç´°ã‚¨ãƒ©ãƒ¼åˆ†é¡ãƒ»æ€§èƒ½çµ±è¨ˆ")
    print(f"ğŸ“„ CSV: {args.csv_path}")
    print(f"ğŸ†” Request ID: {args.request_id}")
    print(f"âš™ï¸  è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«: {args.config}")
    
    # ä¸¦åˆ—åº¦ã®è¡¨ç¤º
    if args.parallel is not None:
        if args.parallel == 1:
            print(f"ğŸ”„ å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†")
        elif args.parallel > 1:
            print(f"ğŸ”„ å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: ä¸¦åˆ—å‡¦ç† ({args.parallel}ä¸¦åˆ—)")
        else:
            print(f"ğŸ”„ å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: è‡ªå‹•è¨­å®š")
    
    # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å‡¦ç†ã®å®Ÿè¡Œ
    processor = ArchiveProcessorTestV4(args.config)
    
    # ã‚¨ãƒ©ãƒ¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
    simulation_args = {}
    if args.simulate_file_lock is not None:
        simulation_args['file_lock'] = args.simulate_file_lock
    if args.simulate_permission is not None:
        simulation_args['permission'] = args.simulate_permission
    if args.simulate_network is not None:
        simulation_args['network'] = args.simulate_network
    if args.simulate_file_missing is not None:
        simulation_args['file_missing'] = args.simulate_file_missing
    if args.simulate_s3_invalid is not None:
        simulation_args['s3_invalid'] = args.simulate_s3_invalid
    
    if simulation_args:
        processor.enable_error_simulation(simulation_args)
    
    exit_code = processor.run(args.csv_path, args.request_id, args.parallel)
    
    sys.exit(exit_code)


if __name__ == "__main__":
    main()error_reason': 'ç©ºã®ãƒ‘ã‚¹'}
            
            invalid_chars = ['<', '>', ':', '"', '|', '?', '*']
            check_path = path[2:] if path.startswith('\\\\') else path
            for char in invalid_chars:
                if char in check_path:
                    return {'valid': False, 'error_reason': f'ä¸æ­£ãªæ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {char}'}
            
            if len(path) > 260:
                return {'valid': False, 'error_reason': f'ãƒ‘ã‚¹ãŒé•·ã™ãã¾ã™: {len(path)} > 260'}
            
            if not os.path.exists(path):
                return {'valid': False, 'error_reason': 'ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“'}
            
            if not os.path.isdir(path):
                return {'valid': False, 'error_reason': 'ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ï¼‰'}
            
            if not os.access(path, os.R_OK):
                return {'valid': False, 'error_reason': 'èª­ã¿å–ã‚Šæ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“'}
            
            return {'valid': True, 'error_reason': None}
            
        except Exception as e:
            return {'valid': False, '