# アーカイブスクリプト仕様書

## 1. 概要

### 1.1 目的
企業内ファイルサーバ上のファイルをAWS S3 Glacier Deep Archiveにアーカイブし、履歴をデータベースに記録するPythonスクリプト。

### 1.2 スクリプト名
`archive_script_main.py`

### 1.3 実行環境
- **Python**: 3.8以上
- **OS**: Windows Server（FSxアクセス用）
- **AWS**: EC2インスタンス（4vCPU、16GBメモリ）

## 2. 機能仕様

### 2.1 主要機能
1. **CSV読み込み・検証**: ディレクトリパスの妥当性チェック
2. **ファイル収集**: 対象ディレクトリ内のファイル列挙
3. **S3アップロード**: Glacier Deep Archiveへの直接転送
4. **アーカイブ後処理**: 空ファイル作成・元ファイル削除
5. **データベース登録**: PostgreSQLへの履歴記録
6. **エラーハンドリング**: 再試行可能なエラーCSV生成

### 2.2 入力仕様

#### 2.2.1 コマンドライン引数
```bash
python archive_script_main.py <csv_path> <request_id> [--config <config_path>]
```

| 引数 | 必須 | 説明 | 例 |
|------|------|------|-----|
| csv_path | ✓ | 対象ディレクトリを記載したCSVファイルパス | `requests.csv` |
| request_id | ✓ | アーカイブ依頼ID | `REQ-2025-001` |
| --config | - | 設定ファイルパス | `config/archive_config.json` |

#### 2.2.2 CSVファイル形式
```csv
Directory Path
\\server\share\project1
\\server\share\project2
C:\local\directory
```

**仕様**:
- **エンコーディング**: UTF-8-SIG
- **ヘッダー**: "Directory Path"または"Path"を含む行（自動検出）
- **データ行**: 1行1ディレクトリパス
- **パス形式**: UNCパス（`\\server\share`）またはローカルパス（`C:\path`）

#### 2.2.3 設定ファイル形式
```json
{
    "aws": {
        "region": "ap-northeast-1",
        "s3_bucket": "your-bucket-name",
        "storage_class": "DEEP_ARCHIVE",
        "vpc_endpoint_url": "https://bucket.vpce-xxx.s3.region.vpce.amazonaws.com"
    },
    "database": {
        "host": "localhost",
        "port": 5432,
        "database": "archive_system",
        "user": "postgres",
        "password": "password",
        "timeout": 30
    },
    "request": {
        "requester": "12345678"
    },
    "file_server": {
        "archived_suffix": "_archived",
        "exclude_extensions": [".tmp", ".lock", ".bak"]
    },
    "processing": {
        "max_file_size": 10737418240,
        "chunk_size": 8388608,
        "retry_count": 3
    },
    "logging": {
        "log_directory": "logs",
        "log_level": "INFO"
    }
}
```

### 2.3 出力仕様

#### 2.3.1 ログファイル
- **場所**: `logs/archive_YYYYMMDD_HHMMSS.log`
- **レベル**: DEBUG（ファイル）、INFO（コンソール）
- **形式**: `YYYY-MM-DD HH:MM:SS - logger_name - LEVEL - message`

#### 2.3.2 エラーCSVファイル
**CSV検証エラー**: `logs/{元ファイル名}_csv_retry_YYYYMMDD_HHMMSS.csv`
```csv
Directory Path
\\invalid\path1
\\invalid\path2
```

**アーカイブエラー**: `logs/{元ファイル名}_archive_retry_YYYYMMDD_HHMMSS.csv`
```csv
Directory Path
\\failed\directory1
\\failed\directory2
```

#### 2.3.3 空ファイル
- **場所**: 元ファイルと同じディレクトリ
- **命名**: `{元ファイル名}_archived`
- **内容**: 完全に空（0バイト）

#### 2.3.4 データベースレコード
```sql
INSERT INTO archive_history (
    request_id,           -- コマンドライン引数
    requester,           -- 設定ファイル（8桁社員番号）
    request_date,        -- 実行時刻
    original_file_path,  -- 元ファイルフルパス
    s3_path,            -- s3://bucket/key形式
    archive_date,       -- 実行時刻
    file_size           -- バイト数
);
```

## 3. 処理フロー仕様

### 3.1 メイン処理フロー
```
1. 設定ファイル読み込み
2. ログ初期化
3. CSV読み込み・検証
4. ファイル収集
5. S3アップロード
6. アーカイブ後処理
7. データベース登録
8. 統計情報出力
```

### 3.2 CSV検証処理
```
入力: CSVファイルパス
出力: (有効ディレクトリリスト, エラー項目リスト)

1. UTF-8-SIG エンコーディングで読み込み
2. ヘッダー行の自動検出・スキップ
3. 各行のパス検証
   - 長さチェック（3文字以上、260文字以下）
   - 不正文字チェック（<>:"|?*）
   - 存在チェック
   - ディレクトリチェック
   - 読み取り権限チェック
4. エラー発生時も処理継続
5. エラーCSV生成（エラーがある場合）
```

### 3.3 S3アップロード処理
```
入力: ファイル情報リスト
出力: アップロード結果リスト

1. boto3 S3クライアント初期化
   - VPCエンドポイント設定
   - 接続テスト実行
2. ストレージクラス検証・自動変換
3. 各ファイルの処理
   - S3キー生成（サーバ名ベース）
   - リトライ付きアップロード（最大3回）
   - 指数バックオフ
4. 進捗ログ出力
```

### 3.4 アーカイブ後処理
```
入力: S3アップロード結果リスト
出力: アーカイブ完了結果リスト

成功ファイルのみ処理:
1. 空ファイル作成（{元ファイル名}_archived）
2. 空ファイル作成成功確認
3. 元ファイル削除
4. 元ファイル削除成功確認
5. 失敗時の自動クリーンアップ
```

### 3.5 データベース登録処理
```
入力: アーカイブ完了結果リスト
出力: なし

アーカイブ完了ファイルのみ処理:
1. PostgreSQL接続
2. トランザクション開始
3. バッチ挿入（executemany）
4. コミット
5. 接続クローズ
```

## 4. エラーハンドリング仕様

### 4.1 エラー分類

| エラー種別 | 処理継続 | リトライ | エラーCSV |
|-----------|---------|---------|-----------|
| CSV読み込みエラー | × | - | - |
| CSV検証エラー | ✓ | - | ✓ |
| S3接続エラー | × | - | - |
| S3アップロードエラー | ✓ | ✓ | ✓ |
| ファイルアクセスエラー | ✓ | × | ✓ |
| データベース接続エラー | ✓ | × | - |

### 4.2 リトライ仕様
- **対象**: S3アップロードエラー
- **回数**: 最大3回
- **間隔**: 指数バックオフ（2^n秒）
- **除外**: FileNotFoundError、PermissionError

### 4.3 ログレベル仕様

| レベル | 用途 | 例 |
|--------|------|-----|
| DEBUG | デバッグ情報 | S3キー生成詳細 |
| INFO | 処理進捗 | ファイル収集完了 |
| WARNING | 警告 | アップロード失敗（リトライ中） |
| ERROR | エラー | ディレクトリが存在しません |

## 5. パフォーマンス仕様

### 5.1 処理能力
- **想定ファイル数**: 10,000-20,000ファイル/月
- **最大ファイルサイズ**: 10GB（設定可能）
- **並行処理**: なし（シーケンシャル処理）

### 5.2 メモリ使用量
- **ファイル情報**: 約100バイト/ファイル
- **想定最大**: 約2MB（20,000ファイル時）

### 5.3 実行時間目安
- **1,000ファイル**: 約5-10分
- **10,000ファイル**: 約50-100分

## 6. セキュリティ仕様

### 6.1 認証
- **AWS**: IAMロール認証
- **PostgreSQL**: ユーザー名・パスワード認証

### 6.2 通信暗号化
- **S3**: HTTPS（VPCエンドポイント経由）
- **PostgreSQL**: SSL（設定による）

### 6.3 権限
- **ファイルサーバ**: 読み取り・削除権限
- **S3**: PutObject権限
- **PostgreSQL**: INSERT権限

## 7. 制約事項

### 7.1 システム制約
- **並行実行**: 不可（同一ディレクトリでのファイル競合回避）
- **ネットワーク**: VPCエンドポイント必須
- **文字エンコーディング**: UTF-8のみサポート

### 7.2 ファイル制約
- **最大パス長**: 260文字（Windows制限）
- **禁止文字**: `<>:"|?*`
- **除外拡張子**: `.tmp`, `.lock`, `.bak`（設定可能）

### 7.3 運用制約
- **復元履歴**: 記録しない
- **削除されたファイル**: 復旧不可
- **日本語ファイル名**: S3では問題なし、psql表示で注意

## 8. 戻り値仕様

### 8.1 終了コード

| コード | 意味 | 説明 |
|--------|------|------|
| 0 | 正常終了 | 全処理完了 |
| 1 | 異常終了 | 致命的エラー発生 |

### 8.2 統計情報
```
=== 処理統計 ===
処理時間: 0:05:23.123456
CSV検証エラー数: 2
総ファイル数: 1000
成功ファイル数: 998
失敗ファイル数: 2
総ファイルサイズ: 1,234,567,890 bytes
```

## 9. 依存関係

### 9.1 Pythonパッケージ
```
boto3>=1.26.0
psycopg2-binary>=2.9.0
```

### 9.2 システム要件
- **Python**: 3.8以上
- **AWS CLI**: 設定済み（認証情報）
- **PostgreSQL**: 接続可能
- **FSx**: マウント済み

## 10. 設定可能項目

### 10.1 主要設定
| 項目 | デフォルト | 説明 |
|------|-----------|------|
| max_file_size | 10GB | 最大ファイルサイズ |
| retry_count | 3 | S3アップロードリトライ回数 |
| archived_suffix | "_archived" | 空ファイルサフィックス |
| log_level | "INFO" | ログレベル |
| timeout | 30 | DB接続タイムアウト（秒） |

### 10.2 AWS設定
- **storage_class**: 自動変換対応（GLACIER_DEEP_ARCHIVE → DEEP_ARCHIVE）
- **vpc_endpoint_url**: VPCエンドポイント必須

## 11. 注意事項

### 11.1 データ整合性
- **アーカイブ成功**: S3アップロード + 空ファイル作成 + DB登録が全て成功
- **部分失敗**: 各段階での失敗は個別に記録・ログ出力

### 11.2 運用上の注意
- **元ファイル削除**: 取り消し不可
- **Glacier復元**: 12-48時間要
- **日本語表示**: pgAdmin推奨（psqlは文字化けの可能性）

### 11.3 障害対応
- **CSV検証エラー**: 再試行用CSVで修正後実行
- **S3エラー**: 権限・ネットワーク確認
- **DB接続エラー**: アーカイブ自体は成功（手動DB登録可能）